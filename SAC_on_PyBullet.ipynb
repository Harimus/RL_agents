{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SAC_on_PyBullet.ipynb","provenance":[{"file_id":"1VZpktZA7-8e9G8UC8EeN-LU7m9FCwa4n","timestamp":1609320383000}],"collapsed_sections":[],"authorship_tag":"ABX9TyPD2I8zUz8XMAUa7PbvR5R9"},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"756ed299835648d9881ce3e9da13e86b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cd4c1e77a6584892b0cea41356e1b8c4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f1d2c5c6db94437a82e8d36435a800c3","IPY_MODEL_789d090617ea4440a31f5f8674865521"]}},"cd4c1e77a6584892b0cea41356e1b8c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f1d2c5c6db94437a82e8d36435a800c3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_51698467e4a74e539676492f90d3eeea","_dom_classes":[],"description":"Step: 559864 | Reward: 0.000000:   0%","_model_name":"FloatProgressModel","bar_style":"","max":440779,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":643,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_577a08f42f1b4ae6ad90b18a8453b203"}},"789d090617ea4440a31f5f8674865521":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e0b228d00a61483a973059e2eec7aca9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 643/441k [02:07&lt;24:18:43, 5.03it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9693cf894bbf4390b381ed02bc965ec1"}},"51698467e4a74e539676492f90d3eeea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"577a08f42f1b4ae6ad90b18a8453b203":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e0b228d00a61483a973059e2eec7aca9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9693cf894bbf4390b381ed02bc965ec1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mEfjToP88sZ8","executionInfo":{"status":"ok","timestamp":1610512234212,"user_tz":-540,"elapsed":19259,"user":{"displayName":"Dan Lillrank","photoUrl":"","userId":"18380904930839814012"}},"outputId":"f81d4a55-83dc-4f6d-8726-31cda919a183"},"source":["!pip install pybullet\n","!pip install tqdm"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pybullet\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/39/c56526c130f092d0123c471c1a749edf45cb74e97b4cdf6a5230a0ce4054/pybullet-3.0.8-cp36-cp36m-manylinux1_x86_64.whl (76.6MB)\n","\u001b[K     |████████████████████████████████| 76.6MB 54kB/s \n","\u001b[?25hInstalling collected packages: pybullet\n","Successfully installed pybullet-3.0.8\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"id":"7G48IkH84_ur","executionInfo":{"status":"ok","timestamp":1610512666348,"user_tz":-540,"elapsed":451384,"user":{"displayName":"Dan Lillrank","photoUrl":"","userId":"18380904930839814012"}},"outputId":"1193d1c3-7f80-4d35-98d3-6c20d8fb1ec4"},"source":["import torch\n","import gym\n","import pybullet_envs\n","from gym import envs\n","from torch.optim import Adam\n","import random\n","from copy import deepcopy\n","from collections import deque\n","import torch.nn as nn\n","import numpy as np\n","from torch.distributions import Distribution, Independent, Normal, TransformedDistribution\n","from torch.distributions.transforms import AffineTransform, TanhTransform\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm_notebook as tqdm\n","DTYPE = torch.float64\n","torch.set_default_dtype(DTYPE)\n","\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","images_dir = '/content/gdrive/My Drive/SAC_IMG/'\n","\n","\n","fig, ax = plt.subplots()\n","def plot(step, reward, title, epoch=1000000, steps=[], rewards=[]):\n","    steps.append(step)\n","    rewards.append(reward)\n","    ax.plot(steps, rewards, 'b-')\n","    ax.set_title(title)\n","    ax.set_xlabel('Steps')\n","    ax.set_ylabel('Rewards')\n","    ax.set_xlim((0, epoch))\n","    ax.set_ylim((-100, 1000))\n","    fig.savefig(f\"{images_dir}\"+title + '.png')\n","\n","LOG_MIN = -20\n","LOG_MAX = 2\n","\n","\n","def fcnn_policy(layer_sizes, activation_function=nn.Tanh, output_activation=nn.Identity):\n","    \"\"\"\n","    This is a basic fully connected neural network, used as an default for the basic model\n","    @param layer_sizes: a list of integers that specify each hidden layers size\n","    @param activation_function: Specify the activation function for the hidden layers.\n","    @param output_activation: The activation function for the output layer.\n","    @return: nn.Sequential\n","    \"\"\"\n","    nn_layers = []\n","    for j in range(len(layer_sizes)-1):\n","        activation = activation_function if j < len(layer_sizes)-2 else output_activation\n","        nn_layers += [nn.Linear(layer_sizes[j], layer_sizes[j+1]), activation()]\n","    return nn.Sequential(*nn_layers)  #Star \"unpacks\" the list, in to args for the function\n","\n","\n","class TanhNormal(Distribution):\n","    \"\"\"Copied from Kaixhi\"\"\"\n","    def __init__(self, loc, scale):\n","        super().__init__()\n","        self.normal = Independent(Normal(loc, scale), 1)\n","\n","    def sample(self):\n","        return torch.tanh(self.normal.sample())\n","\n","    # samples with re-parametrization trick (differentiable)\n","    def rsample(self):\n","        return torch.tanh(self.normal.rsample())\n","\n","    # Calculates log probability of value using the change-of-variables technique\n","    # (uses log1p = log(1 + x) for extra numerical stability)\n","    def log_prob(self, value):\n","        inv_value = (torch.log1p(value) - torch.log1p(-value)) / 2  # artanh(y)\n","        # log p(f^-1(y)) + log |det(J(f^-1(y)))|\n","        return self.normal.log_prob(inv_value) - torch.log1p(-value.pow(2) + 1e-6).sum(dim=1)\n","\n","    @property\n","    def mean(self):\n","        return torch.tanh(self.normal.mean)\n","\n","    def get_std(self):\n","        return self.normal.stddev\n","\n","\n","class SoftActor(nn.Module):\n","    def __init__(self, observation_dim, action_dim, hidden_layer_size=(256, 256),\n","                 activation_function=nn.Tanh, output_activation=nn.Identity,\n","                 action_scale=1.0, action_loc=0.0):\n","        super().__init__()\n","        \"\"\"The network have output size of action_dim*2 because output of this network are \n","        divided in to the mu and log_std component of a gaussian distribution\"\"\"\n","        self.network = fcnn_policy([observation_dim]+list(hidden_layer_size)+[2*action_dim],\n","                                   activation_function=activation_function, output_activation=output_activation)\n","        self.scale = action_scale\n","        self.loc = action_loc\n","\n","    def forward(self, state, mean_action=False):\n","        mu, log_std = self.network(state).chunk(2, dim=-1)\n","        log_std = torch.clamp(log_std, LOG_MIN, LOG_MAX)  # to make it not too random/deterministic\n","        normal = TransformedDistribution(Independent(Normal(mu, log_std.exp()), 1),\n","                                         [TanhTransform(), AffineTransform(loc=self.loc, scale=self.scale)])\n","        if mean_action:\n","            return self.loc*torch.tanh(mu) + self.scale\n","        return normal\n","\n","\n","class Critic(nn.Module):\n","    def __init__(self, observation_dim, action_dim=None, hidden_layer_size=(256, 256),\n","                 activation_function=nn.Tanh, output_activation=nn.Identity):\n","        super().__init__()\n","        network_architecture = [observation_dim + (action_dim if action_dim is not None else 0)] + list(hidden_layer_size) + [1]\n","        self.network = fcnn_policy(network_architecture,\n","                                   activation_function=activation_function, output_activation= output_activation)\n","\n","    def forward(self, state, action=None):\n","        if action is None:\n","            value = self.network(state)\n","        else:\n","            value = self.network(torch.cat([state, action], dim=-1))\n","        return value.squeeze(dim=-1)\n","\n","\n","class SoftActorCritic(nn.Module):\n","\n","    def __init__(self, observation_dim, action_dim, hidden_layer_size=(256, 256),\n","                 activation_function=nn.ReLU, output_activation=nn.Identity, action_scale=1.0, action_loc=0.0):\n","        super().__init__()\n","        self.actor = SoftActor(observation_dim=observation_dim, action_dim=action_dim,\n","                               hidden_layer_size=hidden_layer_size, activation_function=activation_function,\n","                               output_activation=output_activation, action_scale=action_scale, action_loc=action_loc)\n","        self.critic_v = Critic(observation_dim=observation_dim, action_dim=None,\n","                               hidden_layer_size=hidden_layer_size, activation_function=activation_function,\n","                               output_activation=output_activation)\n","\n","        self.critic_q1 = Critic(observation_dim=observation_dim, action_dim=action_dim,\n","                                hidden_layer_size=hidden_layer_size, activation_function=activation_function,\n","                                output_activation=output_activation)\n","\n","        self.critic_q2 = Critic(observation_dim=observation_dim, action_dim=action_dim,\n","                                hidden_layer_size=hidden_layer_size, activation_function=activation_function,\n","                                output_activation=output_activation)\n","        self.target_critic_v = deepcopy(self.critic_v)\n","        for param in self.target_critic_v.parameters():\n","            param.requires_grad = False\n","        self.name = \"Soft Actor Critic\"\n","\n","    def copy_target_critic_v(self):\n","        self.target_critic_v = deepcopy(self.critic_v)\n","        for param in self.target_critic_v.parameters():\n","            param.requires_grad = False\n","\n","    def forward(self, state, mean_action=False):\n","        return self.actor(state, mean_action=mean_action)\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"1KaF3TeQNOz9","executionInfo":{"status":"ok","timestamp":1610512666351,"user_tz":-540,"elapsed":451384,"user":{"displayName":"Dan Lillrank","photoUrl":"","userId":"18380904930839814012"}}},"source":["\n","def save_model_checkpoint(model: torch.nn.Module, optimizer: torch.optim.Optimizer, criterion=0, epochs=0, replay_buffer=[], steps=[], rewards=[], filename='./saved_model.pth'):\n","    torch.save({\n","        'epoch': epochs,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': criterion,\n","        'replay_buffer': replay_buffer,\n","        'steps': steps,\n","        'rewards': rewards\n","                },  filename)\n","\n","\n","def load_model_checkpoint(model: torch.nn.Module, optimizer: torch.optim.Optimizer, filename='./saved_model.pth'):\n","    checkpoint = torch.load(filename)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    return model, optimizer, checkpoint['loss'], checkpoint['epoch'], checkpoint['replay_buffer'], checkpoint['steps'], checkpoint['rewards']\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"AY7JngLj_VlQ","executionInfo":{"status":"ok","timestamp":1610512666629,"user_tz":-540,"elapsed":451659,"user":{"displayName":"Dan Lillrank","photoUrl":"","userId":"18380904930839814012"}}},"source":["DTYPE = torch.float64\n","torch.set_default_dtype(DTYPE)\n","\n","def sac(policy=SoftActorCritic, epoch=1000000, gamma=0.99, polyak=0.995,\n","        actor_lr=3e-4, critic_lr=3e-4, alpha=0.2, buffer_size=1000000,\n","        off_policy_batch_size=1000, reward_scale=1, initial_exploration=10000,\n","        update_interval=1, test_interval=1000, load_agent=False, save_agent=False):\n","\n","    envname = \"HumanoidBulletEnv-v0\" #'MountainCarContinuous-v0' #'Pendulum-v0\n","    env = gym.make(envname)\n","\n","\n","    observation_space = env.observation_space.shape[0]\n","    action_space = env.action_space.shape[0]\n","    # Scaling of action\n","    max_action = env.action_space.high[0]\n","    min_action = env.action_space.low[0]\n","    clamping = 1e-10\n","    scaling_factor = (env.action_space.high[0] - env.action_space.low[0])/2\n","    scaling_const = (env.action_space.high[0] + env.action_space.low[0])/2\n","\n","    def scale_action(act):\n","        scaled_act= scaling_factor*act + scaling_const\n","        return act\n","        #return np.clip(scaling_factor*act + scaling_const,\n","        #                env.action_space.low[0], env.action_space.high[0])\n","\n","    def test(actor, render=False):\n","        with torch.no_grad():\n","            actor.eval()\n","            state, total_reward, done = env.reset(), 0, False\n","\n","            while not done:\n","                action = actor(torch.as_tensor(state, dtype=DTYPE), mean_action=True)\n","                #env.render() if render is True else _\n","                state, reward, done, _ = env.step(scale_action(action.numpy()))\n","                total_reward += reward\n","            #env.close() #crashes mujoco_py\n","            #plot_mountain_cart(agent) if render else _\n","            actor.train()\n","        return total_reward\n","\n","    agent = policy(observation_space, action_space, hidden_layer_size=(256, 256), activation_function=nn.ReLU,\n","                   action_scale=scaling_factor, action_loc=scaling_const)\n","    actor_optimizer = Adam(agent.actor.parameters(), lr=actor_lr)\n","    critic_q_optimizer = Adam(list(agent.critic_q1.parameters())+list(agent.critic_q2.parameters()), lr=critic_lr)\n","    critic_v_optimizer = Adam(agent.critic_v.parameters(), lr=critic_lr)\n","    replay_buffer = deque(maxlen=buffer_size)\n","\n","    # Loading parameters from previous run\n","    prev_loss = 0\n","    prev_epoch = 0\n","    steps, rewards = [], []\n","    if load_agent:\n","        agent.actor, actor_optimizer, prev_loss, prev_epoch, replay_buffer, steps, rewards = load_model_checkpoint(agent.actor, actor_optimizer, filename=f\"{images_dir}\"+\"/save_points/SAC_actor.pth\") #temporary workaround\n","        agent.critic_v, critic_v_optimizer, _, _, _, _, _ = load_model_checkpoint(agent.critic_v, critic_v_optimizer, filename=f\"{images_dir}\"+\"/save_points/SAC_critic_v.pth\")\n","        agent.critic_q1, critic_q_optimizer, _, _, _, _, _ = load_model_checkpoint(agent.critic_q1, critic_q_optimizer, filename=f\"{images_dir}\"+\"/save_points/SAC_critic_q1.pth\")\n","        agent.critic_q2, critic_q_optimizer, _, _, _, _, _ = load_model_checkpoint(agent.critic_q2, critic_q_optimizer, filename=f\"{images_dir}\"+\"/save_points/SAC_critic_q2.pth\")\n","        agent.copy_target_critic_v()  # make sure target_critic is also updated\n","\n","    def update(episodes, update_target=False):\n","        states = torch.as_tensor([ep[0] for ep in episodes], dtype=DTYPE)\n","        actions = torch.stack([ep[1] for ep in episodes])\n","        rewards = torch.as_tensor([ep[2] for ep in episodes], dtype=DTYPE)\n","        next_states = torch.as_tensor([ep[3] for ep in episodes], dtype=DTYPE)\n","        is_done = torch.as_tensor([ep[-1] for ep in episodes], dtype=DTYPE)\n","        # Target for V\n","        pi = agent.actor(states)\n","        sample_actions = torch.clamp(pi.rsample(), min_action + clamping, max_action - clamping)\n","        entropy_component = alpha * pi.log_prob(sample_actions)\n","        critic_q = torch.min(agent.critic_q2(states, sample_actions),\n","                             agent.critic_q1(states, sample_actions)).detach()\n","        target_v = critic_q - entropy_component.detach()\n","\n","        # Update V\n","        critic_v_loss = (agent.critic_v(states) - target_v).pow(2).mean()\n","        critic_v_optimizer.zero_grad()\n","        critic_v_loss.backward()\n","        critic_v_optimizer.step()\n","        # Target for Q\n","        target_q = rewards + gamma * (1-is_done) * agent.target_critic_v(next_states)\n","        # Update Q\n","        critic_q_loss = (agent.critic_q1(states, actions) - target_q).pow(2).mean() + \\\n","                        (agent.critic_q2(states, actions) - target_q).pow(2).mean()\n","        critic_q_optimizer.zero_grad()\n","        critic_q_loss.backward()\n","        critic_q_optimizer.step()\n","\n","\n","        # Update Policy\n","        target_pi = torch.min(agent.critic_q2(states, sample_actions), agent.critic_q1(states, sample_actions))\n","        loss_pi = (entropy_component - target_pi).mean()\n","        actor_optimizer.zero_grad()\n","        loss_pi.backward()\n","        actor_optimizer.step()\n","        if torch.isnan(loss_pi).any().item() or torch.isnan(critic_q_loss).any().item() or torch.isnan(critic_v_loss).any().item():\n","            print(\"what the hell\")\n","        if torch.isinf(loss_pi).any().item() or torch.isinf(critic_q_loss).any().item() or torch.isinf(critic_v_loss).any().item():\n","            print(\"what the hell\")\n","\n","        # Update target V\n","        for V_param, target_V_param in zip(agent.critic_v.network.parameters(), agent.target_critic_v.network.parameters()):\n","            target_V_param.data = polyak * target_V_param.data + (1 - polyak) * V_param.data\n","\n","    total_reward = 0\n","    i = 0\n","\n","    try:\n","        state, done = env.reset(), False\n","        progress_bar = tqdm(range(1+prev_epoch, epoch+1), unit_scale=1, smoothing=0)\n","        for i in progress_bar:\n","            with torch.no_grad():\n","                if i < initial_exploration:\n","                    # uniform distribution action taken for better exploration at the beginning roughtly 10%\n","                    action = torch.tensor(np.random.rand(action_space)*2 - 1)\n","                else:\n","                    action = agent(torch.as_tensor(state, dtype=torch.float64)).sample()\n","                next_state, reward, done, _ = env.step(scale_action(action.numpy()))\n","                replay_buffer.append([state, action, reward_scale*reward, next_state, done])\n","                if done:\n","                    state = env.reset()\n","                else:\n","                    state = next_state\n","            if i > initial_exploration and i % update_interval == 0:\n","                batch = random.sample(replay_buffer, off_policy_batch_size)\n","                update(batch, True)\n","            if i > initial_exploration and i % test_interval == 0:\n","                total_reward = test(agent, render=False)# if i % (epoch/10) else False)\n","                plot(i, total_reward, \"SoftActorCritic\", epoch, steps=steps, rewards=rewards)\n","            progress_bar.set_description('Step: %i | Reward: %f' % (i, total_reward))\n","        #Save all agents\n","        print(\"Saving trained network parameters to save_points....\")\n","        if save_agent:\n","            save_model_checkpoint(agent.actor, actor_optimizer, criterion=total_reward, epochs=i, replay_buffer=replay_buffer, steps=steps, rewards=rewards, filename=f\"{images_dir}\"+\"/save_points/SAC_actor.pth\")\n","            save_model_checkpoint(agent.critic_v, critic_v_optimizer, filename=f\"{images_dir}\"+\"/save_points/SAC_critic_v.pth\")\n","            save_model_checkpoint(agent.critic_q1, critic_q_optimizer, filename=f\"{images_dir}\"+\"/save_points/SAC_critic_q1.pth\")\n","            save_model_checkpoint(agent.critic_q2, critic_q_optimizer, filename=f\"{images_dir}\"+\"/save_points/SAC_critic_q2.pth\")\n","        return agent\n","\n","    except KeyboardInterrupt:\n","        #Actor saves all additional values, skip for critic\n","        if save_agent:\n","            print(\"Saving trained network parameters to save_points....\")\n","            save_model_checkpoint(agent.actor, actor_optimizer, criterion=total_reward, epochs=i, replay_buffer=replay_buffer, steps=steps, rewards=rewards, filename=f\"{images_dir}\"+\"/save_points/SAC_actor.pth\")\n","            save_model_checkpoint(agent.critic_v, critic_v_optimizer, filename=f\"{images_dir}\"+\"/save_points/SAC_critic_v.pth\")\n","            save_model_checkpoint(agent.critic_q1, critic_q_optimizer, filename=f\"{images_dir}\"+\"/save_points/SAC_critic_q1.pth\")\n","            save_model_checkpoint(agent.critic_q2, critic_q_optimizer, filename=f\"{images_dir}\"+\"/save_points/SAC_critic_q2.pth\")\n","        print(\"Done.\")\n","        return agent\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":118,"referenced_widgets":["756ed299835648d9881ce3e9da13e86b","cd4c1e77a6584892b0cea41356e1b8c4","f1d2c5c6db94437a82e8d36435a800c3","789d090617ea4440a31f5f8674865521","51698467e4a74e539676492f90d3eeea","577a08f42f1b4ae6ad90b18a8453b203","e0b228d00a61483a973059e2eec7aca9","9693cf894bbf4390b381ed02bc965ec1"]},"id":"1gBGtIJ9_17i","outputId":"5a201437-1907-483c-a6ec-edcf758a370a"},"source":["ag = None\n","if __name__ == '__main__':\n","    ag = sac(load_agent=True, save_agent=True)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:109: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"756ed299835648d9881ce3e9da13e86b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=440779.0), HTML(value='')))"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ZrRNlkR6WpHY"},"source":["    from matplotlib import animation\n","    from matplotlib import pyplot as plt\n","    %matplotlib nbagg\n","    with torch.no_grad():\n","        ag.eval()\n","        atleast = 1000\n","        i = 0\n","        envname = \"HumanoidBulletEnv-v0\" #'MountainCarContinuous-v0' #'Pendulum-v0\n","        env = gym.make(envname)\n","        state, total_reward, done = env.reset(), 0, False\n","        frames = []\n","        while not done or i < atleast:\n","            if done:\n","                print(\"total reward: \", total_reward)\n","                state, total_reward, done = env.reset(), 0, False\n","            frames.append(env.render(mode='rgb_array'))\n","            action = ag(torch.as_tensor(state, dtype=DTYPE)).sample()\n","            state, reward, done, _ = env.step(action.numpy())\n","            total_reward += reward\n","            i+=1\n","    fig = plt.gcf()\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","\n","    def animate(i):\n","        patch.set_data(frames[i])\n","\n","    anim = animation.FuncAnimation(fig, animate, frames = len(frames), interval=50)\n","    anim\n","    anim.save('latest_run.mp4')\n"],"execution_count":null,"outputs":[]}]}